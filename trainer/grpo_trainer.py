import os
import textwrap
import warnings
from collections import defaultdict, deque
from collections.abc import Sized
from contextlib import nullcontext
from typing import Any, Callable, Optional, Union

import torch
import torch.utils.data
import transformers
from accelerate.utils import broadcast_object_list, gather, gather_object, is_peft_model, set_seed
from datasets import Dataset, IterableDataset
from packaging import version
from torch import nn
from torch.utils.data import Sampler
from transformers import (
    AutoModelForCausalLM,
    AutoModelForSequenceClassification,
    AutoTokenizer,
    GenerationConfig,
    PreTrainedModel,
    PreTrainedTokenizerBase,
    Trainer,
    TrainerCallback,
    is_wandb_available,
)
from transformers.integrations.deepspeed import is_deepspeed_zero3_enabled
from transformers.utils import is_liger_kernel_available, is_peft_available

from ..data_utils import apply_chat_template, is_conversational, maybe_apply_chat_template
from ..extras.profiling import profiling_context, profiling_decorator
from ..extras.vllm_client import VLLMClient
from ..import_utils import is_deepspeed_available, is_rich_available, is_vllm_available
from ..models import create_reference_model, prepare_deepspeed, unwrap_model_for_generation
from .callbacks import SyncRefModelCallback
from .grpo_config import GRPOConfig
from .utils import (
    generate_model_card,
    get_comet_experiment_url,
    pad,
    print_prompt_completions_sample,
    selective_log_softmax,
)


if is_deepspeed_available():
    import deepspeed

if is_peft_available():
    from peft import PeftConfig, get_peft_model

if is_liger_kernel_available():
    from liger_kernel.chunked_loss import LigerFusedLinearGRPOLoss

if is_wandb_available():
    import wandb

#æˆ‘ä»¬ç§°ä¹‹ä¸ºå¥–åŠ±å‡½æ•°çš„æ˜¯ä¸€ä¸ªå¯è°ƒç”¨å‡½æ•°ï¼Œå®ƒæ¥å—ä¸€ç³»åˆ—æç¤ºå’Œå®Œæˆï¼Œå¹¶è¿”å›ä¸€ä¸ªå¥–åŠ±ã€‚å½“å®ƒæ˜¯å­—ç¬¦ä¸²æ—¶ï¼Œå®ƒæ˜¯ä¸€ä¸ªæ¨¡å‹IDï¼Œå› æ­¤å®ƒè¢«åŠ è½½ä¸ºé¢„è®­ç»ƒæ¨¡å‹ã€‚
RewardFunc = Union[str, PreTrainedModel, Callable[[list, list], list[float]]]


class RepeatRandomSampler(Sampler):
    """
    ä»¥ç»“æ„åŒ–æ–¹å¼é‡å¤æ•°æ®é›†ç´¢å¼•çš„é‡‡æ ·å™¨ã€‚

    Args:
        data_source (`Sized`):
            Dataset to sample from.
        mini_repeat_count (`int`):
            Number of times to repeat each index per batch.
        batch_size (`int`, *optional*, defaults to `1`):
            Number of unique indices per batch.
        repeat_count (`int`, *optional*, defaults to `1`):
            Number of times to repeat the full sampling process.
        seed (`int` or `None`, *optional*, defaults to `None`):
            Random seed for reproducibility (only affects this sampler).

    Example:
    ```python
    >>> sampler = RepeatRandomSampler(["a", "b", "c", "d", "e", "f", "g"], mini_repeat_count=2, batch_size=3, repeat_count=4)
    >>> list(sampler)
    [4, 4, 3, 3, 0, 0,
     4, 4, 3, 3, 0, 0,
     4, 4, 3, 3, 0, 0,
     4, 4, 3, 3, 0, 0,

     1, 1, 2, 2, 6, 6,
     1, 1, 2, 2, 6, 6,
     1, 1, 2, 2, 6, 6,
     1, 1, 2, 2, 6, 6]
    ```

    ```txt
    mini_repeat_count = 3
          -   -   -
         [0,  0,  0,  1,  1,  1,  2,  2,  2,  3,  3,  3,      |
          4,  4,  4,  5,  5,  5,  6,  6,  6,  7,  7,  7,      |
          8,  8,  8,  9,  9,  9, 10, 10, 10, 11, 11, 11,      |
                                                                repeat_count = 2
          0,  0,  0,  1,  1,  1,  2,  2,  2,  3,  3,  3,      |
          4,  4,  4,  5,  5,  5,  6,  6,  6,  7,  7,  7,      |
          8,  8,  8,  9,  9,  9, 10, 10, 10, 11, 11, 11, ...] |
          ---------   ---------   ---------   ---------
           ---------   ---------   ---------   ---------
            ---------   ---------   ---------   ---------
                         batch_size = 12
    ```
    """

    def __init__(
        self,
        data_source: Sized,
        mini_repeat_count: int,
        batch_size: int = 1,
        repeat_count: int = 1,
        seed: Optional[int] = None,
    ):
        self.data_source = data_source
        self.mini_repeat_count = mini_repeat_count
        self.batch_size = batch_size
        self.repeat_count = repeat_count
        self.num_samples = len(data_source)
        self.seed = seed
        self.generator = torch.Generator()  # Create a local random generator
        if seed is not None:
            self.generator.manual_seed(seed)

    def __iter__(self):
        # E.g., [2, 4, 3, 1, 0, 6, 5] (num_samples = 7)
        indexes = torch.randperm(self.num_samples, generator=self.generator).tolist()

        #    [2, 4, 3, 1, 0, 6, 5]
        # -> [[2, 4, 3], [1, 0, 6], [5]]  (batch_size = 3)
        indexes = [indexes[i : i + self.batch_size] for i in range(0, len(indexes), self.batch_size)]

        #    [[2, 4, 3], [1, 0, 6], [5]]
        # -> [[2, 4, 3], [1, 0, 6]]
        indexes = [chunk for chunk in indexes if len(chunk) == self.batch_size]

        for chunk in indexes:
            for _ in range(self.repeat_count):
                for index in chunk:
                    for _ in range(self.mini_repeat_count):
                        yield index

    def __len__(self) -> int:
        return self.num_samples * self.mini_repeat_count * self.repeat_count


# torch.nanstd doesn't exist, so we define it here
def nanstd(tensor: torch.Tensor) -> torch.Tensor:
    """
    è®¡ç®—å¼ é‡çš„æ ‡å‡†åå·®ï¼Œå¿½ç•¥NaNã€‚æ­¤å‡½æ•°ä»…æ”¯æŒ1Då¼ é‡ã€‚

    Args:
        tensor (`torch.Tensor`):
            Input tensor of shape `(N,)`.

    Returns:
        `torch.Tensor`:
            Standard deviation of the tensor, ignoring NaNs.
    """
    variance = torch.nanmean((tensor - torch.nanmean(tensor, keepdim=True)) ** 2)  # Compute variance ignoring NaNs
    count = torch.sum(~torch.isnan(tensor))  # Count of non-NaN values
    variance *= count / (count - 1)  # Bessel's correction
    return torch.sqrt(variance)


class GRPOTrainer(Trainer):
    """
    ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰æ–¹æ³•è®­ç»ƒã€‚è¯¥ç®—æ³•æœ€åˆæ˜¯åœ¨ paper [DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models](https://huggingface.co/papers/2402.03300).

    Example:

    ```python
    from datasets import load_dataset
    from trl import GRPOTrainer

    dataset = load_dataset("trl-lib/tldr", split="train")

    def reward_func(completions, **kwargs):
        # Dummy reward function that rewards completions with more unique letters.
        return [float(len(set(completion))) for completion in completions]

    trainer = GRPOTrainer(
        model="Qwen/Qwen2-0.5B-Instruct",
        reward_funcs=reward_func,
        train_dataset=dataset,
    )

    trainer.train()
    ```

    Args:
        model (`Union[str, PreTrainedModel]`):
è¦è®­ç»ƒçš„æ¨¡å‹ã€‚
å¯ä»¥æ˜¯ï¼š
-ä¸€ä¸ªå­—ç¬¦ä¸²ï¼Œæ˜¯huggingface.coä¸Šæ¨¡å‹ä»“åº“å†…æ‰˜ç®¡çš„é¢„è®­ç»ƒæ¨¡å‹çš„*model id*ï¼Œæˆ–è€…æ˜¯åŒ…å«ä½¿ç”¨[`~transformers.PreTrainingModel.save_presetrained`]ä¿å­˜çš„æ¨¡å‹æƒé‡çš„*ç›®å½•*çš„è·¯å¾„ï¼Œä¾‹å¦‚â€œâ€/
my_model_ç›®å½•/â€œâ€ã€‚
ä½¿ç”¨[`~transformers.AutoModelForCausalLM.from_petrained`]åŠ è½½æ¨¡å‹ï¼Œå¹¶åœ¨`args.model_init_kwargs`ä¸­åŒ…å«å…³é”®å­—å‚æ•°ã€‚
-[`~transformers.PreTrainingModel`]å¯¹è±¡ã€‚
ä»…æ”¯æŒå› æœè¯­è¨€æ¨¡å‹ã€‚
reward_funcsï¼ˆ`Union[RewardFuncï¼Œlist[RewardFunction]]ï¼‰ï¼šç”¨äºè®¡ç®—å¥–åŠ±çš„å¥–åŠ±å‡½æ•°ã€‚
ä¸ºäº†è®¡ç®—å¥–åŠ±ï¼Œæˆ‘ä»¬è°ƒç”¨æ‰€æœ‰å¸¦æœ‰æç¤ºå’Œå®Œæˆçš„å¥–åŠ±å‡½æ•°ï¼Œå¹¶å°†å¥–åŠ±ç›¸åŠ ã€‚
å¯ä»¥æ˜¯ï¼š
-å•ä¸€å¥–åŠ±åŠŸèƒ½ï¼Œä¾‹å¦‚ï¼š
-å­—ç¬¦ä¸²ï¼šhuggingface.coä¸Šæ¨¡å‹ä»“åº“å†…æ‰˜ç®¡çš„é¢„è®­ç»ƒæ¨¡å‹çš„*model ID*ï¼Œæˆ–åŒ…å«ä½¿ç”¨[`~transformers.PreTrainedModel.save_presetrained`]ä¿å­˜çš„æ¨¡å‹æƒé‡çš„*ç›®å½•*çš„è·¯å¾„ï¼Œä¾‹å¦‚â€œâ€/
my_model_ç›®å½•/â€œâ€ã€‚
ä½¿ç”¨[`~transformers.AutoModelForSequenceClassification.from_petrained`]åŠ è½½æ¨¡å‹ï¼Œå…¶ä¸­`num_labels=1 `å’Œ`args.model_init_kwargs`ä¸­çš„å…³é”®å­—å‚æ•°ã€‚
-[`~transformers.PreTrainingModel`]å¯¹è±¡ï¼šä»…æ”¯æŒåºåˆ—åˆ†ç±»æ¨¡å‹ã€‚
-è‡ªå®šä¹‰å¥–åŠ±å‡½æ•°ï¼šè¯¥å‡½æ•°æä¾›æç¤ºå’Œç”Ÿæˆçš„è¡¥å…¨ï¼Œä»¥åŠæ•°æ®é›†ä¸­çš„ä»»ä½•å…¶ä»–åˆ—ã€‚
å®ƒåº”è¯¥è¿”å›ä¸€ä¸ªå¥–åŠ±åˆ—è¡¨ã€‚
å½“å¥–åŠ±ä¸é€‚ç”¨äºè¿™äº›æ ·æœ¬æ—¶ï¼Œè‡ªå®šä¹‰å¥–åŠ±å‡½æ•°ä¹Ÿå¯ä»¥è¿”å›Noneã€‚
è¿™å¯¹äºå¤šä»»åŠ¡è®­ç»ƒéå¸¸æœ‰ç”¨ï¼Œå…¶ä¸­ä¸åŒçš„å¥–åŠ±å‡½æ•°é€‚ç”¨äºä¸åŒç±»å‹çš„æ ·æœ¬ã€‚
å½“å¥–åŠ±å‡½æ•°ä¸ºæ ·æœ¬è¿”å›Noneæ—¶ï¼Œè¯¥å¥–åŠ±å‡½æ•°å°†è¢«æ’é™¤åœ¨è¯¥æ ·æœ¬çš„å¥–åŠ±è®¡ç®—ä¹‹å¤–ã€‚
æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…[ä½¿ç”¨è‡ªå®šä¹‰å¥–åŠ±åŠŸèƒ½]ï¼ˆ#ä½¿ç”¨è‡ªå®šä¹‰å¥–åŠ±å‡½æ•°ï¼‰ã€‚
-å¥–åŠ±å‡½æ•°åˆ—è¡¨ï¼Œå…¶ä¸­æ¯ä¸ªé¡¹ç›®å¯ä»¥ç‹¬ç«‹åœ°æ˜¯ä¸Šè¿°ä»»ä½•ç±»å‹ã€‚
å…è®¸åœ¨åˆ—è¡¨ä¸­æ··åˆä¸åŒç±»å‹ï¼ˆä¾‹å¦‚ï¼Œå­—ç¬¦ä¸²æ¨¡å‹IDå’Œè‡ªå®šä¹‰å¥–åŠ±å‡½æ•°ï¼‰ã€‚
argsï¼ˆ[`GRPOConfig`]ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`None `ï¼‰ï¼šæ­¤åŸ¹è®­å™¨çš„é…ç½®ã€‚
å¦‚æœä¸ºâ€œæ— â€ï¼Œåˆ™ä½¿ç”¨é»˜è®¤é…ç½®ã€‚
train_datasetï¼ˆ[`~datas.dataset`]æˆ–[`~dataset.IterableDataset`ï¼‰ï¼š
ç”¨äºè®­ç»ƒçš„æ•°æ®é›†ã€‚
å®ƒå¿…é¡»åŒ…å«ä¸€åˆ—â€œæç¤ºâ€ã€‚
æ•°æ®é›†ä¸­çš„ä»»ä½•å…¶ä»–åˆ—éƒ½å°†è¢«å¿½ç•¥ã€‚
æ ·æœ¬çš„æ ¼å¼å¯ä»¥æ˜¯ï¼š

-[Standard]ï¼ˆdataset_formats#Standardï¼‰ï¼šæ¯ä¸ªæ ·æœ¬éƒ½åŒ…å«çº¯æ–‡æœ¬ã€‚
-[ä¼šè¯]ï¼ˆdataset_formats#conventionalï¼‰ï¼šæ¯ä¸ªæ ·æœ¬éƒ½åŒ…å«ç»“æ„åŒ–æ¶ˆæ¯ï¼ˆä¾‹å¦‚ï¼Œè§’è‰²å’Œå†…å®¹ï¼‰ã€‚
eval_adsetï¼ˆ[`~datas.dataset`]ï¼Œ[`~dataset.IterableDataset`]]æˆ–`dict[strï¼ŒUnion[datasetï¼ŒIterableDataset]]ï¼‰ï¼šç”¨äºè®¡ç®—çš„æ•°æ®é›†ã€‚
å®ƒå¿…é¡»æ»¡è¶³ä¸â€œtrain_datasetâ€ç›¸åŒçš„è¦æ±‚ã€‚
processing_classï¼ˆ[`~transformers.PreTrainingTokenBase`]ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸ºâ€œæ— â€ï¼‰ï¼šç”¨äºå¤„ç†æ•°æ®çš„å¤„ç†ç±»ã€‚
è¡¬å«ä¾§å¿…é¡»è®¾ç½®ä¸ºâ€œå·¦ä¾§â€ã€‚
å¦‚æœä¸ºâ€œNoneâ€ï¼Œåˆ™ä½¿ç”¨[`~transformers.AutoTokenizer.from_petrained`ä»æ¨¡å‹åç§°åŠ è½½å¤„ç†ç±»ã€‚
reward_processing_classesï¼ˆ`Union[PreTrainedTokenizerBaseï¼Œlist[PreTraindTokenizerBase]]ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`Noneâ€™ï¼‰ï¼šä¸`reward_funcs'ä¸­æŒ‡å®šçš„å¥–åŠ±å‡½æ•°å¯¹åº”çš„å¤„ç†ç±»ã€‚
å¯ä»¥æ˜¯ï¼š
-å•ä¸ªå¤„ç†ç±»ï¼šå½“`reward_funcs`åªåŒ…å«ä¸€ä¸ªå¥–åŠ±å‡½æ•°æ—¶ä½¿ç”¨ã€‚
-å¤„ç†ç±»åˆ—è¡¨ï¼šå¿…é¡»ä¸â€œreward_funcsâ€ä¸­å¥–åŠ±å‡½æ•°çš„é¡ºåºå’Œé•¿åº¦åŒ¹é…ã€‚
å¦‚æœè®¾ç½®ä¸ºâ€œNoneâ€ï¼Œæˆ–è€…å¦‚æœåˆ—è¡¨ä¸­ä¸[`~transformers.PreTrainedModel`]å¯¹åº”çš„å…ƒç´ ä¸ºâ€œNoneâ€ï¼Œåˆ™ä½¿ç”¨[`~transformers.AutoTokenizer.from_petrained']è‡ªåŠ¨åŠ è½½æ¨¡å‹çš„æ ‡è®°å™¨ã€‚
å¯¹äºâ€œreward_funcsâ€ä¸­å±äºè‡ªå®šä¹‰å¥–åŠ±å‡½æ•°ï¼ˆä¸æ˜¯[`~transformers.PreTrainingModel`]ï¼‰çš„å…ƒç´ ï¼Œå°†å¿½ç•¥â€œreward_processing_classesâ€ä¸­çš„ç›¸åº”æ¡ç›®ã€‚
callbacksï¼ˆ[`~transformers.TrainerCallback`]åˆ—è¡¨ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸ºâ€œæ— â€ï¼‰ï¼šç”¨äºè‡ªå®šä¹‰è®­ç»ƒå¾ªç¯çš„å›è°ƒåˆ—è¡¨ã€‚
å°†è¿™äº›æ·»åŠ åˆ°[æ­¤å¤„]ä¸­è¯¦ç»†è¯´æ˜çš„é»˜è®¤å›è°ƒåˆ—è¡¨ä¸­(
https://huggingface.co/docs/transformers/main_classes/callback
). 
å¦‚æœè¦åˆ é™¤ä½¿ç”¨çš„é»˜è®¤å›è°ƒä¹‹ä¸€ï¼Œè¯·ä½¿ç”¨[`~transformers.Trainer.remove_callback`]æ–¹æ³•ã€‚
ä¼˜åŒ–å™¨ï¼ˆâ€œtuple[torch.optimi.Optimizerï¼Œtorch.ooptim.lr_scheduler.LambdaLR]â€ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸ºâ€œï¼ˆNoneï¼ŒNoneï¼‰â€ï¼‰ï¼šåŒ…å«è¦ä½¿ç”¨çš„ä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨çš„å…ƒç»„ã€‚
å°†é»˜è®¤ä¸ºæ¨¡å‹ä¸Šçš„[`AdamW`]å®ä¾‹å’Œç”±`args `æ§åˆ¶çš„[`get_liner_schedule_with_warmup`]ç»™å‡ºçš„è°ƒåº¦å™¨ã€‚
peft_configï¼ˆ[`~peft.PeftConfig `]ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸ºâ€œæ— â€ï¼‰ï¼šç”¨äºåŒ…è£…æ¨¡å‹çš„pefté…ç½®ã€‚
å¦‚æœä¸ºâ€œæ— â€ï¼Œåˆ™æ¨¡å‹æœªè¢«åŒ…è£…ã€‚
    """

    _tag_names = ["trl", "grpo"]

    def __init__(
        self,
        model: Union[str, PreTrainedModel],
        reward_funcs: Union[RewardFunc, list[RewardFunc]],
        args: Optional[GRPOConfig] = None,
        train_dataset: Optional[Union[Dataset, IterableDataset]] = None,
        eval_dataset: Optional[Union[Dataset, IterableDataset, dict[str, Union[Dataset, IterableDataset]]]] = None,
        processing_class: Optional[PreTrainedTokenizerBase] = None,
        reward_processing_classes: Optional[Union[PreTrainedTokenizerBase, list[PreTrainedTokenizerBase]]] = None,
        callbacks: Optional[list[TrainerCallback]] = None,
        optimizers: tuple[Optional[torch.optim.Optimizer], Optional[torch.optim.lr_scheduler.LambdaLR]] = (None, None),
        peft_config: Optional["PeftConfig"] = None,
    ):
        # Args
        if args is None:
            model_name = model if isinstance(model, str) else model.config._name_or_path
            model_name = model_name.split("/")[-1]
            args = GRPOConfig(f"{model_name}-GRPO")

        # Models
        # Trained model
        model_init_kwargs = args.model_init_kwargs or {}
        if isinstance(model, str):
            model_id = model
            torch_dtype = model_init_kwargs.get("torch_dtype")
            if isinstance(torch_dtype, torch.dtype) or torch_dtype == "auto" or torch_dtype is None:
                pass  # torch_dtype is already a torch.dtype or "auto" or None
            elif isinstance(torch_dtype, str):  # it's a str, but not "auto"
                torch_dtype = getattr(torch, torch_dtype)
                model_init_kwargs["torch_dtype"] = torch_dtype
            else:
                raise ValueError(
                    "Invalid `torch_dtype` passed to `GRPOConfig`. Expected either 'auto' or a string representing "
                    f"a `torch.dtype` (e.g., 'float32'), but got {torch_dtype}."
                )
            # å¦‚æœå¯ç”¨äº†æ¢¯åº¦æ£€æŸ¥ç‚¹ï¼Œåˆ™ç¦ç”¨ç¼“å­˜ï¼ˆä¸æ”¯æŒï¼‰
            model_init_kwargs["use_cache"] = (
                False if args.gradient_checkpointing else model_init_kwargs.get("use_cache")
            )
            model = AutoModelForCausalLM.from_pretrained(model, **model_init_kwargs)
        else:
            model_id = model.config._name_or_path
            if args.model_init_kwargs is not None:
                raise ValueError(
                    "You passed `model_init_kwargs` to the `GRPOConfig`, but your model is already instantiated. "
                    "This argument can only be used when the `model` argument is a string."
                )

        if peft_config is not None:
            if not is_peft_available():
                raise ImportError("PEFT is required to use `peft_config`. Run `pip install peft`.")
            model = get_peft_model(model, peft_config)

        # å¦‚æœéœ€è¦ï¼Œå¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹
        if args.gradient_checkpointing:
            model = self._enable_gradient_checkpointing(model, args)

        # Reference model
        self.beta = args.beta
        if self.beta == 0.0:
            # å¦‚æœbetaä¸º0.0ï¼Œåˆ™ä¸éœ€è¦å‚è€ƒæ¨¡å‹
            self.ref_model = None
        elif is_deepspeed_zero3_enabled():
            self.ref_model = AutoModelForCausalLM.from_pretrained(model_id, **model_init_kwargs)
        elif is_peft_model(model):
            # å¦‚æœä½¿ç”¨PEFTï¼Œåˆ™ä¸éœ€è¦å‚è€ƒæ¨¡å‹ï¼Œå› ä¸ºå¯ä»¥ç¦ç”¨é€‚é…å™¨ä»¥æ¢å¤åˆ°åˆå§‹æ¨¡å‹ã€‚
            self.ref_model = None
        else:
            # å¦‚æœæ²¡æœ‰æä¾›PEFTé…ç½®ï¼Œè¯·åŸºäºåˆå§‹æ¨¡å‹åˆ›å»ºå‚è€ƒæ¨¡å‹ã€‚
            self.ref_model = create_reference_model(model)

        # Processing class
        if processing_class is None:
            processing_class = AutoTokenizer.from_pretrained(model.config._name_or_path, padding_side="left")

        # Reward functions
        if not isinstance(reward_funcs, list):
            reward_funcs = [reward_funcs]
        for i, reward_func in enumerate(reward_funcs):
            if isinstance(reward_func, str):
                reward_funcs[i] = AutoModelForSequenceClassification.from_pretrained(
                    reward_func, num_labels=1, **model_init_kwargs
                )
        self.reward_funcs = reward_funcs

        # Reward weights
        if args.reward_weights is not None:
            if len(args.reward_weights) != len(reward_funcs):
                raise ValueError(
                    f"Number of reward weights ({len(args.reward_weights)}) must match number of reward "
                    f"functions ({len(reward_funcs)})"
                )
            self.reward_weights = torch.tensor(args.reward_weights, dtype=torch.float32)
        else:
            self.reward_weights = torch.ones(len(reward_funcs), dtype=torch.float32)

        # Reward processing class
        if reward_processing_classes is None:
            reward_processing_classes = [None] * len(reward_funcs)
        elif not isinstance(reward_processing_classes, list):
            reward_processing_classes = [reward_processing_classes]
        else:
            if len(reward_processing_classes) != len(reward_funcs):
                raise ValueError("The number of reward processing classes must match the number of reward functions.")

        for i, (reward_processing_class, reward_func) in enumerate(zip(reward_processing_classes, reward_funcs)):
            if isinstance(reward_func, PreTrainedModel):
                if reward_processing_class is None:
                    reward_processing_class = AutoTokenizer.from_pretrained(reward_func.config._name_or_path)
                if reward_processing_class.pad_token_id is None:
                    reward_processing_class.pad_token = reward_processing_class.eos_token
                # å¥–åŠ±æ¨¡å‹è®¡ç®—è¾“å…¥åºåˆ—ä¸­æœ€æ–°éå¡«å……ä»¤ç‰Œçš„å¥–åŠ±ã€‚
                # å› æ­¤ï¼Œå°†å¡«å……ä»¤ç‰ŒIDè®¾ç½®ä¸ºå¤„ç†ç±»çš„å¡«å……ä»¤ç‰ŒIDéå¸¸é‡è¦ã€‚
                reward_func.config.pad_token_id = reward_processing_class.pad_token_id
                reward_processing_classes[i] = reward_processing_class
        self.reward_processing_classes = reward_processing_classes

        # Data collator
        def data_collator(features):  # No data collation is needed in GRPO
            return features

        # Training arguments
        self.max_prompt_length = args.max_prompt_length
        self.max_completion_length = args.max_completion_length  # = |o_i| in the GRPO paper
        self.num_generations = args.num_generations  # = G in the GRPO paper
        self.temperature = args.temperature
        self.top_p = args.top_p
        self.top_k = args.top_k
        self.min_p = args.min_p
        self.repetition_penalty = args.repetition_penalty
        self.use_vllm = args.use_vllm
        self.use_liger_loss = args.use_liger_loss

        # Datasets
        if (
            isinstance(train_dataset, IterableDataset)
            or isinstance(eval_dataset, IterableDataset)
            or (
                isinstance(eval_dataset, dict) and any(isinstance(ds, IterableDataset) for ds in eval_dataset.values())
            )
        ):
            # See https://github.com/huggingface/trl/issues/3213
            raise NotImplementedError(
                "Iterable datasets are not yet supported in GRPOTrainer. Please use a standard dataset instead."
            )

        # Multi-step
        self.num_iterations = args.num_iterations  # = ğœ‡ in the GRPO paper
        self.epsilon_low = args.epsilon
        self.epsilon_high = args.epsilon_high if args.epsilon_high is not None else args.epsilon
        # è·Ÿè¸ªè¿­ä»£æ¬¡æ•°ï¼ˆæ­£å‘+åå‘ä¼ é€’ï¼‰ï¼ŒåŒ…æ‹¬ä¸€ä¸ªæ¢¯åº¦ç´¯ç§¯å‘¨æœŸå†…çš„è¿­ä»£æ¬¡æ•°
        self._step = 0
        # ç¼“å†²æ‰¹å¤„ç†ï¼Œä»¥ä¾¿åœ¨å¤šä¸ªæ›´æ–°ä¸­é‡ç”¨ç”Ÿæˆçš„è¾“å‡ºã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…
        # #â€œ_get_train_samplerâ€å’Œâ€œ_prepare_inputsâ€ã€‚
        self._buffered_inputs = [None] * args.gradient_accumulation_steps

        # è®­ç»ƒä½¿ç”¨ä¸­çš„å…ƒç´ æ•°é‡æ¥ä¼°è®¡FLOPï¼ˆæµ®ç‚¹è¿ç®—ï¼‰çš„æ•°é‡
        # #ä¸é”®â€œinput_idsâ€å…³è”çš„è¾“å…¥å¼ é‡ã€‚
        # ç„¶è€Œï¼Œåœ¨GRPOä¸­ï¼Œé‡‡æ ·æ•°æ®ä¸åŒ…æ‹¬
        # #â€œinput_idsâ€é”®ã€‚
        # ç›¸åï¼Œå¯ç”¨çš„å¯†é’¥æ˜¯â€œæç¤ºâ€ã€‚
        # å› æ­¤ï¼ŒåŸ¹è®­å¸ˆå‘å‡ºè­¦å‘Šï¼š
        # #æ— æ³•ä¼°è®¡è¾“å…¥çš„ä»¤ç‰Œæ•°ï¼Œå°†ä¸è®¡ç®—æµ®ç‚¹è¿ç®—
        # #å–æ¶ˆæ­¤è­¦å‘Šï¼Œæˆ‘ä»¬å°†æ¨¡å‹çš„â€œwarnings_issuedâ€å­—å…¸ä¸­çš„â€œestimate_tokensâ€é”®è®¾ç½®ä¸ºTrueã€‚
        # #è¿™ç›¸å½“äºä¸€ä¸ªæ ‡å¿—ï¼Œè¡¨ç¤ºè­¦å‘Šå·²ç»å‘å‡ºã€‚
        model.warnings_issued["estimate_tokens"] = True

        if self.use_liger_loss:
            if not is_liger_kernel_available():
                raise ImportError(
                    "Liger is required to use `liger_loss` as the GRPO loss. Run `pip install liger-kernel`."
                )
            if is_peft_model(model):
                raise ValueError("Liger loss is not supported with a PEFT model.")

            self.liger_grpo_loss = LigerFusedLinearGRPOLoss(
                beta=self.beta,
                epsilon_low=self.epsilon_low,
                epsilon_high=self.epsilon_high,
                temperature=self.temperature,
                use_ref_model=self.ref_model is not None,
            )

        super().__init__(
            model=model,
            args=args,
            data_collator=data_collator,
            train_dataset=train_dataset,
            eval_dataset=eval_dataset,
            processing_class=processing_class,
            callbacks=callbacks,
            optimizers=optimizers,
        )

        # åˆå§‹åŒ–æŒ‡æ ‡
        self._metrics = {"train": defaultdict(list), "eval": defaultdict(list)}
        self._total_train_tokens = 0
        self.log_completions = args.log_completions
        self.num_completions_to_print = args.num_completions_to_print
        # maxlenè®¾ç½®ä¸ºæ¯ä¸€æ­¥å‘å‰ä¼ é€’çš„æ€»æ¬¡æ•°ã€‚`maxlen`çš„å€¼ç¡®ä¿æˆ‘ä»¬åªè®°å½•æœ€åçš„ä¼˜åŒ–æ­¥éª¤ã€‚
        maxlen = self.accelerator.num_processes * args.per_device_train_batch_size * args.gradient_accumulation_steps
        self._textual_logs = {
            "prompt": deque(maxlen=maxlen),
            "completion": deque(maxlen=maxlen),
            "rewards": defaultdict(lambda: deque(maxlen=maxlen)),
        }

        # æ£€æŸ¥per_device_train/eval_batch_size*numè¿›ç¨‹æ˜¯å¦å¯ä»¥é™¤ä»¥ä»£æ•°
        num_processes = self.accelerator.num_processes
        global_batch_size = args.per_device_train_batch_size * num_processes
        possible_values = [n_gen for n_gen in range(2, global_batch_size + 1) if (global_batch_size) % n_gen == 0]
        if self.num_generations < 2:
            raise ValueError(
                f"GRPO requires at least 2 generations per prompt to calculate the advantages. "
                f"You provided {self.num_generations}, which is less than the minimum required."
            )
        if self.num_generations not in possible_values:
            raise ValueError(
                f"The global train batch size ({num_processes} x {args.per_device_train_batch_size}) must be evenly "
                f"divisible by the number of generations per prompt ({self.num_generations}). Given the current train "
                f"batch size, the valid values for the number of generations are: {possible_values}."
            )
        if self.args.eval_strategy != "no":
            global_batch_size = args.per_device_eval_batch_size * num_processes
            possible_values = [n_gen for n_gen in range(2, global_batch_size + 1) if (global_batch_size) % n_gen == 0]
            if self.num_generations not in possible_values:
                raise ValueError(
                    f"The global eval batch size ({num_processes} x {args.per_device_eval_batch_size}) must be evenly "
                    f"divisible by the number of generations per prompt ({self.num_generations}). Given the current "
                    f"eval batch size, the valid values for the number of generations are: {possible_values}."
                )

        # ç¡®ä¿æ¯ä¸ªè¿›ç¨‹éƒ½æ”¶åˆ°ä¸€ä¸ªå”¯ä¸€çš„ç§å­ï¼Œä»¥é˜²æ­¢åœ¨ç”Ÿæˆæ—¶é‡å¤å®Œæˆ
        # #å¦‚æœnum_generationè¶…è¿‡per_device_train_batch_sizeï¼Œåˆ™ä½¿ç”¨å˜å‹å™¨ã€‚
        # å¦‚æœæˆ‘ä»¬ä½¿ç”¨vLLMï¼Œæˆ‘ä»¬å¯ä»¥è·³è¿‡å®ƒï¼Œä½†æ˜¯
        # #åœ¨æ‰€æœ‰æƒ…å†µä¸‹è®¾ç½®å®ƒéƒ½æ›´å®‰å…¨ã€‚
        set_seed(args.seed, device_specific=True)

        if self.use_vllm:
            if not is_vllm_available():
                raise ImportError(
                    "vLLM is not available and `use_vllm` is set to True. Please install vLLM with "
                    "`pip install vllm` to use it."
                )

            if self.accelerator.is_main_process:
                self.vllm_client = VLLMClient(
                    args.vllm_server_host, args.vllm_server_port, connection_timeout=args.vllm_server_timeout
                )

            # vLLMç‰¹å®šçš„é‡‡æ ·å‚æ•°
            self.guided_decoding_regex = args.vllm_guided_decoding_regex

            self._last_loaded_step = -1  # æ ‡è®°ä»¥é¿å…åœ¨æ¢¯åº¦ç´¯ç§¯è¿‡ç¨‹ä¸­æ— ç”¨çš„åŠ è½½

            # ä½¿ç”¨vLLMæ—¶ï¼Œä¸»è¿›ç¨‹è´Ÿè´£åŠ è½½æ¨¡å‹æƒé‡ã€‚
            # è¿™å¯èƒ½ä¼šå¯¼è‡´è¿›ç¨‹å»åŒæ­¥ï¼Œä¼¼ä¹ä¼šå¯¼è‡´åˆå§‹åŒ–æœŸé—´DeepSpeedæŒ‚èµ·ã€‚
            # ä¸ºäº†é˜²æ­¢è¿™ç§æƒ…å†µï¼Œæˆ‘ä»¬åœ¨vLLMå®Œå…¨åˆå§‹åŒ–ååŒæ­¥æ‰€æœ‰è¿›ç¨‹ã€‚
            self.accelerator.wait_for_everyone()
        else:
            self.generation_config = GenerationConfig(
                max_new_tokens=self.max_completion_length,
                do_sample=True,
                pad_token_id=processing_class.pad_token_id,
                bos_token_id=processing_class.bos_token_id,
                eos_token_id=processing_class.eos_token_id,
                temperature=self.temperature,
                top_p=self.top_p,
                top_k=self.top_k,
                min_p=self.min_p,
                repetition_penalty=self.repetition_penalty,
                cache_implementation=args.cache_implementation,
            )

        # æ¢¯åº¦ç´¯ç§¯éœ€è¦æŒ‰æ¯”ä¾‹æŸå¤±ã€‚
        # é€šå¸¸ï¼Œçˆ¶ç±»ä¸­çš„æŸå¤±ç¼©æ”¾å–å†³äºæ¨¡å‹æ¥å—ä¸æŸå¤±ç›¸å…³çš„kwargsã€‚ç”±äºæˆ‘ä»¬è®¡ç®—è‡ªå·±çš„æŸå¤±ï¼Œå› æ­¤æ­¤æ£€æŸ¥æ— å…³ç´§è¦ã€‚
        # æˆ‘ä»¬è®¾ç½®self.model_accepts_loss_kwargsè®¾ç½®ä¸ºFalseä»¥å¯ç”¨ç¼©æ”¾ã€‚
        self.model_accepts_loss_kwargs = False

        # Add tags to the model
        self.model.add_model_tags(self._tag_names)

        if self.ref_model is not None:
            if self.is_deepspeed_enabled:
                self.ref_model = prepare_deepspeed(self.ref_model, self.accelerator)
            else:
                self.ref_model = self.accelerator.prepare_model(self.ref_model, evaluation_mode=True)

        if args.sync_ref_model:
            self.add_callback(SyncRefModelCallback(ref_model=self.ref_model, accelerator=self.accelerator))

        for i, reward_func in enumerate(self.reward_funcs):
            if isinstance(reward_func, PreTrainedModel):
                self.reward_funcs[i] = self.accelerator.prepare_model(reward_func, evaluation_mode=True)

    def _set_signature_columns_if_needed(self):
        # å¦‚æœ`self.args.remove_unused_columns`ä¸ºTrueï¼Œåˆ™åˆ é™¤éç­¾ååˆ—ã€‚
        # #é»˜è®¤æƒ…å†µä¸‹ï¼Œæ­¤æ–¹æ³•è®¾ç½®`self_signature_columnsâ€åˆ°æ¨¡å‹çš„é¢„æœŸè¾“å…¥ã€‚
        # #åœ¨GRPOTrainerä¸­ï¼Œæˆ‘ä»¬å¯¹æ•°æ®è¿›è¡Œé¢„å¤„ç†ï¼Œå› æ­¤ä½¿ç”¨æ¨¡å‹çš„ç­¾ååˆ—æ˜¯è¡Œä¸é€šçš„ã€‚ç›¸åï¼Œæˆ‘ä»¬å°†å®ƒä»¬è®¾ç½®ä¸ºâ€œtraining_stepâ€æ–¹æ³•æ‰€æœŸæœ›çš„åˆ—ï¼Œå› æ­¤è¿›è¡Œäº†é‡å†™ã€‚
        if self._signature_columns is None:
            self._signature_columns = ["prompt"]

    def _get_train_sampler(self) -> Sampler:
        # è¿”å›ä¸€ä¸ªé‡‡æ ·å™¨
        #1.ç¡®ä¿æ¯ä¸ªæç¤ºåœ¨å¤šä¸ªè¿›ç¨‹ä¸­é‡å¤ã€‚è¿™ä¿è¯äº†ç›¸åŒçš„æç¤º
        # åˆ†å¸ƒåˆ°ä¸åŒçš„GPUï¼Œå…è®¸åœ¨æ¯ä¸ªæç¤ºå†…æ­£ç¡®è®¡ç®—å’Œè§„èŒƒå¥–åŠ±
        # é›†å›¢ã€‚è·¨æµç¨‹ä½¿ç”¨ç›¸åŒçš„ç§å­å¯ç¡®ä¿ä¸€è‡´çš„åŠæ—¶åˆ†é…ï¼Œé˜²æ­¢å·®å¼‚
        # åœ¨ç¾¤ä½“å½¢æˆä¸­ã€‚
        #2.å¤šæ¬¡é‡å¤è¯¥æ‰¹å¤„ç†ï¼Œä»¥å…è®¸åœ¨å¤šä¸ªæ›´æ–°ä¸­é‡ç”¨ä»£ã€‚æåˆ°
        # _prepare_inputsï¼ŒæŸ¥çœ‹å¦‚ä½•å­˜å‚¨å’Œé‡ç”¨è¿™äº›ä»£ã€‚

        # åœ¨ä¸‹å›¾ä¸­ï¼Œè¿™äº›å€¼æ˜¯æç¤ºç´¢å¼•ã€‚
        ç¬¬ä¸€è¡Œæ˜¾ç¤ºäº†ç¬¬ä¸€ä¸ªé‡‡æ ·æ‰¹æ¬¡
        # ç¬¬äºŒè¡Œæ˜¾ç¤ºç¬¬äºŒä¸ªé‡‡æ ·æ‰¹æ¬¡ï¼Œä»¥æ­¤ç±»æ¨ã€‚
        #
        #                                     |     GPU 0     |     GPU 1     |     GPU 2    |
        #
        #               global_step   step     <â”€â”€â”€â”€â”€â”€â”€>  num_generations=3
        #                                      <â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€> per_device_train_batch_size=4
        #                â–²   0          0      0   0   0   1   1   1   2   2   2   3   3   3  â”‚
        #  grad_accum=3  â”‚   0          1      4   4   4   5   5   5   6   6   6   7   7   7  â”‚ Generate completions for each prompt
        #                â–¼   0          2      8   8   8   9   9   9  10  10  10  11  11  11  â”‚
        #
        #                    1          3      0   0   0   1   1   1   2   2   2   3   3   3  â”‚ The sampled prompts are the same as in the first iteration
        #                    1          4      4   4   4   5   5   5   6   6   6   7   7   7  â”‚ Reuse the completions (here, once, because num_iterations=2)
        #                    1          5      8   8   8   9   9   9  10  10  10  11  11  11  â”‚
        #
        #                    2          6     12  12  12  13  13  13  14  14  14  15  15  15
        #                    2          7     16  16  16  17  17  17  18  18  18  19  19  19
        #                    2          8     20  20  20  21  21  21  22  22  22  23  23  23
        #                                          ...
        effective_batch_size = (
            self.args.per_device_train_batch_size
            * self.accelerator.num_processes
            * self.args.gradient_accumulation_steps
        )
        return RepeatRandomSampler(
            data_source=self.train_dataset,
            mini_repeat_count=self.num_generations,
            batch_size=effective_batch_size // self.num_generations,
            repeat_count=self.num_iterations,
            seed=self.args.seed,
        )

    def _get_eval_sampler(self, eval_dataset) -> Sampler:
        # æœ‰å…³é‡‡æ ·å™¨çš„è¯´æ˜ï¼Œè¯·å‚é˜…_get_train_samplerã€‚
        return RepeatRandomSampler(
            data_source=eval_dataset,
            mini_repeat_count=self.num_generations,
            seed=self.args.seed,
        )

    def _enable_gradient_checkpointing(self, model: PreTrainedModel, args: GRPOConfig) -> PreTrainedModel:
        """Enables gradient checkpointing for the model."""
        # ç¡®ä¿å·²ç¦ç”¨use_cache
        model.config.use_cache = False

        # åœ¨PEFTçš„åŸºç¡€æ¨¡å‹ä¸Šå¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹
        if is_peft_model(model):
            model.base_model.gradient_checkpointing_enable()
        # ä¸ºéPEFTæ¨¡å‹å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹
        else:
            model.gradient_checkpointing_enable()

        gradient_checkpointing_kwargs = args.gradient_checkpointing_kwargs or {}
        use_reentrant = (
            "use_reentrant" not in gradient_checkpointing_kwargs or gradient_checkpointing_kwargs["use_reentrant"]
        )

        if use_reentrant:
            model.enable_input_require_grads()

        return model

    @profiling_decorator
    def _get_last_hidden_state(self, model, input_ids, attention_mask, logits_to_keep=None):
        # æ‰“å¼€æ¨¡å‹ä»¥è®¿é—®model.model
        unwrapped_model = self.accelerator.unwrap_model(model)
        last_hidden_state = unwrapped_model.model(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state
        last_hidden_state = last_hidden_state[:, :-1, :]  # (B, L-1, H)
        if logits_to_keep is not None:
            last_hidden_state = last_hidden_state[:, -logits_to_keep:, :]  # (B, logits_to_keep, H)
        return last_hidden_state

    # è·å–æ¨¡å‹å’Œå‚è€ƒæ¨¡å‹å®Œæˆçš„æ¯ä¸ªä»¤ç‰Œæ—¥å¿—æ¦‚ç‡
    @profiling_decorator
    def _get_per_token_logps(self, model, input_ids, attention_mask, logits_to_keep):
        # æˆ‘ä»¬åœ¨`logits_to_cheet`ä¸­æ·»åŠ 1ï¼Œå› ä¸ºåºåˆ—çš„æœ€åä¸€ä¸ªlogitsç¨åä¼šè¢«æ’é™¤åœ¨å¤–
        logits = model(input_ids=input_ids, attention_mask=attention_mask, logits_to_keep=logits_to_keep + 1).logits
        logits = logits[:, :-1, :]  # (B, L-1, V), exclude the last logit: it corresponds to the next token pred
        input_ids = input_ids[:, -logits_to_keep:]
        # å¯¹äº<=4.48çš„å˜å‹å™¨ï¼Œlogits_to_headå‚æ•°ä¸å—æ”¯æŒï¼Œå› æ­¤åœ¨è¿™é‡Œæˆ‘ä»¬è‡ªå·±åˆ é™¤logitsã€‚
        # See https://github.com/huggingface/trl/issues/2770
        logits = logits[:, -logits_to_keep:]
        # å°†logitsé™¤ä»¥é‡‡æ ·æ¸©åº¦ã€‚
        # See https://huggingface.co/blog/the_n_implementation_details_of_rlhf_with_ppo#policy-training-implementation-details
        logits = logits / self.temperature
        return selective_log_softmax(logits, input_ids)  # compute logprobs for the input tokens

    @profiling_decorator
    def _move_model_to_vllm(self):
        # å¯¹äºDeepSpeed ZeRO-3ï¼Œæˆ‘ä»¬éœ€è¦åœ¨æ“ä½œå‰æ”¶é›†æ‰€æœ‰å‚æ•°
        deepspeed_plugin = self.accelerator.state.deepspeed_plugin
        zero_stage_3 = deepspeed_plugin is not None and deepspeed_plugin.zero_stage == 3
        gather_if_zero3 = deepspeed.zero.GatheredParameters if zero_stage_3 else nullcontext

        if is_peft_model(self.model):
            # ä½¿ç”¨PEFTå’ŒDeepSpeed ZeRO Stage 3ï¼Œæˆ‘ä»¬å¿…é¡»åœ¨åˆå¹¶ä¹‹å‰ç«‹å³æ”¶é›†å®Œæ•´çš„æ¨¡å‹ï¼Œå› ä¸ºåˆå¹¶
            # #ä¸æ”¯æŒåˆ†ç‰‡æ–¹å¼çš„é€‚é…å™¨ã€‚
            with gather_if_zero3(list(self.model.parameters())):
                self.model.merge_adapter()

                # åœ¨æ”¶é›†å‚æ•°æ—¶æ›´æ–°vLLMæƒé‡
                for name, param in self.model.named_parameters():
                    # ä½¿ç”¨PEFTæ—¶ï¼Œæˆ‘ä»¬éœ€è¦æ¢å¤åŸå§‹å‚æ•°åç§°å¹¶ä¸¢å¼ƒä¸€äº›å‚æ•°
                    name = name.removeprefix("base_model.model.").replace(".base_layer", "")
                    if self.model.prefix in name:
                        continue
                    # ä¿å­˜æ¨¡å—æ—¶ï¼Œåˆ é™¤å…¶å‰ç¼€å¹¶ä¸¢å¼ƒåŸå§‹æ¨¡å—
                    if "original_module" in name:
                        continue
                    name = name.replace("modules_to_save.default.", "")

                    if self.accelerator.is_main_process:
                        self.vllm_client.update_named_param(name, param.data)

                # åœ¨ä»åœ¨æ”¶é›†å‚æ•°æ—¶å–æ¶ˆåˆå¹¶é€‚é…å™¨
                self.model.unmerge_adapter()
                # é€€å‡ºä¸Šä¸‹æ–‡æ—¶ï¼Œå‚æ•°å°†è‡ªåŠ¨é‡æ–°åˆ†åŒº
        else:
            # å¯¹äºéPEFTæ¨¡å‹ï¼Œåªéœ€å•ç‹¬æ”¶é›†å’Œæ›´æ–°æ¯ä¸ªå‚æ•°ã€‚
            for name, param in self.model.named_parameters():
                with gather_if_zero3([param]):
                    if self.accelerator.is_main_process:
                        self.vllm_client.update_named_param(name, param.data)

        # é‡ç½®ä¸»è¿›ç¨‹ä¸Šçš„ç¼“å­˜
        if self.accelerator.is_main_process:
            self.vllm_client.reset_prefix_cache()

    @profiling_decorator
    def _prepare_inputs(self, inputs: dict[str, Union[torch.Tensor, Any]]) -> dict[str, Union[torch.Tensor, Any]]:
        mode = "eval" if self.control.should_evaluate else "train"
        if mode == "train":
            buffer_index = self._step % self.args.gradient_accumulation_steps
            buffered_inputs = self._buffered_inputs[buffer_index]
            if self.state.global_step % self.num_iterations == 0 or buffered_inputs is None:
                # buffered_inputs=ä»æ£€æŸ¥ç‚¹æ¢å¤æ—¶ä¸ä¼šå‘ç”Ÿä»»ä½•æƒ…å†µ
                inputs = self._generate_and_score_completions(inputs)
                self._buffered_inputs[buffer_index] = inputs
            else:
                inputs = buffered_inputs
            self._step += 1
        else:
            # åœ¨è¯„ä¼°ä¸­ï¼Œæˆ‘ä»¬ä¸ä¼šåœ¨å¤šä¸ªæ›´æ–°ä¹‹é—´é‡ç”¨è¡¥å…¨ï¼Œå› æ­¤æˆ‘ä»¬ä¸éœ€è¦ç¼“å†²è¾“å…¥ã€‚
            inputs = self._generate_and_score_completions(inputs)
        return inputs

    def _generate_and_score_completions(
        self, inputs: dict[str, Union[torch.Tensor, Any]]
    ) -> dict[str, Union[torch.Tensor, Any]]:
        device = self.accelerator.device
        prompts = [x["prompt"] for x in inputs]
        prompts_text = [maybe_apply_chat_template(example, self.processing_class)["prompt"] for example in inputs]
        prompt_inputs = self.processing_class(
            text=prompts_text, return_tensors="pt", padding=True, padding_side="left", add_special_tokens=False
        )
        prompt_inputs = super()._prepare_inputs(prompt_inputs)
        prompt_ids, prompt_mask = prompt_inputs["input_ids"], prompt_inputs["attention_mask"]

        if self.max_prompt_length is not None:
            prompt_ids = prompt_ids[:, -self.max_prompt_length :]
            prompt_mask = prompt_mask[:, -self.max_prompt_length :]

        # ä½¿ç”¨vLLMæˆ–å¸¸è§„ç”Ÿæˆç”Ÿæˆå®Œæˆ
        if self.args.use_vllm:
            # #é¦–å…ˆï¼Œå¦‚æœ‰éœ€è¦ï¼Œç¡®å®šä¸»è¦åŠ è½½æƒé‡
            if self.state.global_step != self._last_loaded_step:
                self._move_model_to_vllm()
                self._last_loaded_step = self.state.global_step

            # ä½¿ç”¨vLLMç”Ÿæˆè¡¥å…¨ï¼šæ”¶é›†æ‰€æœ‰æç¤ºå¹¶åœ¨ä¸»è¿›ç¨‹ä¸­çš„å•ä¸ªè°ƒç”¨ä¸­ä½¿ç”¨å®ƒä»¬
            all_prompts_text = gather_object(prompts_text)
            if self.accelerator.is_main_process:
                # ç”±äºâ€œpromisesâ€åŒ…å«â€œnum_generationsâ€é‡å¤é¡¹ï¼Œæˆ‘ä»¬é¦–å…ˆé‡‡ç”¨å”¯ä¸€çš„æç¤ºï¼Œå¹¶ç”Ÿæˆnum_generationä¸ºæ¯ä¸ªè¾“å‡ºã€‚
                # è¿™æ¯”ä¸ºæ¯ä¸ªå‰¯æœ¬ç”Ÿæˆè¾“å‡ºæ›´å¿«å•ç‹¬æç¤ºã€‚
                ordered_set_of_prompts = all_prompts_text[:: self.num_generations]
                with profiling_context(self, "vLLM.generate"):
                    completion_ids = self.vllm_client.generate(
                        prompts=ordered_set_of_prompts,
                        n=self.num_generations,
                        repetition_penalty=self.repetition_penalty,
                        temperature=self.temperature,
                        top_p=self.top_p,
                        top_k=-1 if self.top_k is None else self.top_k,
                        min_p=0.0 if self.min_p is None else self.min_p,
                        max_tokens=self.max_completion_length,
                        guided_decoding_regex=self.guided_decoding_regex,
                    )
            else:
                completion_ids = [None] * len(all_prompts_text)
            # å°†ä¸»æµç¨‹çš„å®Œæˆæƒ…å†µå¹¿æ’­ç»™æ‰€æœ‰æµç¨‹ï¼Œç¡®ä¿æ¯ä¸ªæµç¨‹éƒ½æ”¶åˆ°å…¶å¯¹åº”åˆ‡ç‰‡ã€‚
            completion_ids = broadcast_object_list(completion_ids, from_process=0)
            process_slice = slice(
                self.accelerator.process_index * len(prompts),
                (self.accelerator.process_index + 1) * len(prompts),
            )
            completion_ids = completion_ids[process_slice]

            # å¡«å……è¡¥å…¨ï¼Œå¹¶å°†å…¶ä¸æç¤ºè¿æ¥èµ·æ¥
            completion_ids = [torch.tensor(ids, device=device) for ids in completion_ids]
            completion_ids = pad(completion_ids, padding_value=self.processing_class.pad_token_id)
            prompt_completion_ids = torch.cat([prompt_ids, completion_ids], dim=1)
        else:
            # å¸¸è§„ç”Ÿæˆè·¯å¾„
            with unwrap_model_for_generation(
                self.model_wrapped, self.accelerator, gather_deepspeed3_params=self.args.ds3_gather_for_generation
            ) as unwrapped_model:
                prompt_completion_ids = unwrapped_model.generate(
                    prompt_ids, attention_mask=prompt_mask, generation_config=self.generation_config
                )

            # è®¡ç®—æç¤ºé•¿åº¦å¹¶æå–å®ŒæˆID
            prompt_length = prompt_ids.size(1)
            prompt_ids = prompt_completion_ids[:, :prompt_length]
            completion_ids = prompt_completion_ids[:, prompt_length:]

        # åœ¨ç¬¬ä¸€ä¸ªEOSä»£å¸ä¹‹åå±è”½æ‰€æœ‰å†…å®¹
        is_eos = completion_ids == self.processing_class.eos_token_id
        eos_idx = torch.full((is_eos.size(0),), is_eos.size(1), dtype=torch.long, device=device)
        eos_idx[is_eos.any(dim=1)] = is_eos.int().argmax(dim=1)[is_eos.any(dim=1)]
        sequence_indices = torch.arange(is_eos.size(1), device=device).expand(is_eos.size(0), -1)
        completion_mask = (sequence_indices <= eos_idx.unsqueeze(1)).int()

        # å°†prompt_maskä¸completion_maskè¿æ¥èµ·æ¥è¿›è¡Œlogitè®¡ç®—
        attention_mask = torch.cat([prompt_mask, completion_mask], dim=1)  # (B, P+C)

        logits_to_keep = completion_ids.size(1)  # æˆ‘ä»¬åªéœ€è¦è®¡ç®—å®Œæˆä»¤ç‰Œçš„logits

        with torch.no_grad():
            # å½“ä½¿ç”¨num_iterations==1æ—¶ï¼Œold_per_token_logps==per_token_logsï¼Œæ‰€ä»¥æˆ‘ä»¬å¯ä»¥è·³è¿‡å®ƒ
            # #åœ¨è¿™é‡Œè¿›è¡Œè®¡ç®—ï¼Œå¹¶æ”¹ç”¨per_token_logs.detachï¼ˆï¼‰ã€‚
            if self.num_iterations > 1:
                old_per_token_logps = self._get_per_token_logps(
                    self.model, prompt_completion_ids, attention_mask, logits_to_keep
                )
            else:
                old_per_token_logps = None

            if self.beta == 0.0:
                ref_per_token_logps = None
            elif self.ref_model is not None:
                ref_per_token_logps = self._get_per_token_logps(
                    self.ref_model, prompt_completion_ids, attention_mask, logits_to_keep
                )
            else:
                with self.accelerator.unwrap_model(self.model).disable_adapter():
                    ref_per_token_logps = self._get_per_token_logps(
                        self.model, prompt_completion_ids, attention_mask, logits_to_keep
                    )

        # å¯¹ç”Ÿæˆçš„è¡¥å…¨è¿›è¡Œè§£ç 
        completions_text = self.processing_class.batch_decode(completion_ids, skip_special_tokens=True)
        if is_conversational(inputs[0]):
            completions = []
            for prompt, completion in zip(prompts, completions_text):
                bootstrap = prompt.pop()["content"] if prompt[-1]["role"] == "assistant" else ""
                completions.append([{"role": "assistant", "content": bootstrap + completion}])
        else:
            completions = completions_text

        rewards_per_func = torch.zeros(len(prompts), len(self.reward_funcs), device=device)
        for i, (reward_func, reward_processing_class) in enumerate(
            zip(self.reward_funcs, self.reward_processing_classes)
        ):
            if isinstance(reward_func, nn.Module):  # Module instead of PretrainedModel for compat with compiled models
                reward_func_name = f"reward {reward_func.config._name_or_path.split('/')[-1]}"
            else:
                reward_func_name = reward_func.__name__
            with profiling_context(self, reward_func_name):
                if isinstance(
                    reward_func, nn.Module
                ):  #æ¨¡å—è€Œä¸æ˜¯é¢„è®­ç»ƒæ¨¡å‹ï¼Œä»¥ä¸ç¼–è¯‘åçš„æ¨¡å‹å…¼å®¹
                    if is_conversational(inputs[0]):
                        messages = [{"messages": p + c} for p, c in zip(prompts, completions)]
                        texts = [apply_chat_template(x, reward_processing_class)["text"] for x in messages]
                    else:
                        texts = [p + c for p, c in zip(prompts, completions)]
                    reward_inputs = reward_processing_class(
                        text=texts, return_tensors="pt", padding=True, padding_side="right", add_special_tokens=False
                    )
                    reward_inputs = super()._prepare_inputs(reward_inputs)
                    with torch.inference_mode():
                        rewards_per_func[:, i] = reward_func(**reward_inputs).logits[:, 0]  # Shape (B*G,)
                else:
                    # é‡å¤æ‰€æœ‰è¾“å…¥åˆ—ï¼ˆä½†â€œæç¤ºâ€å’Œâ€œå®Œæˆâ€é™¤å¤–ï¼‰ä»¥åŒ¹é…ä»£æ•°
                    keys = [key for key in inputs[0] if key not in ["prompt", "completion"]]
                    reward_kwargs = {key: [example[key] for example in inputs] for key in keys}
                    output_reward_func = reward_func(prompts=prompts, completions=completions, **reward_kwargs)
                    # Convert None values to NaN
                    output_reward_func = [reward if reward is not None else torch.nan for reward in output_reward_func]

                    rewards_per_func[:, i] = torch.tensor(output_reward_func, dtype=torch.float32, device=device)

        # å¦‚æœç»™å®šè¡Œçš„æ‰€æœ‰å¥–åŠ±å‡½æ•°éƒ½è¿”å›Noneï¼Œåˆ™å‘å‡ºè¯¦ç»†è­¦å‘Š
        if torch.isnan(rewards_per_func).all(dim=1).any():
            nan_row_idx = torch.isnan(rewards_per_func).all(dim=1).nonzero(as_tuple=True)[0][0]
            row_reward_kwargs = {key: value[nan_row_idx] for key, value in reward_kwargs.items()}
            row_reward_kwargs["prompt"] = prompts[nan_row_idx]
            row_reward_kwargs["completion"] = completions[nan_row_idx]
            warnings.warn(
                f"All reward functions returned None for the following kwargs: {row_reward_kwargs}. "
                "Please ensure that at least one reward function returns a valid reward."
            )

        # æ”¶é›†æ¯ä¸ªåŠŸèƒ½çš„å¥–åŠ±ï¼šè¿™éƒ¨åˆ†è‡³å…³é‡è¦ï¼Œå› ä¸ºæ¯ä¸ªç»„çš„å¥–åŠ±éƒ½æ˜¯æ ‡å‡†åŒ–çš„
        # #å®Œå·¥å¯èƒ½åˆ†å¸ƒåœ¨å„ä¸ªæµç¨‹ä¸­
        rewards_per_func = gather(rewards_per_func)

        # å¯¹æ¯ä¸ªå¥–åŠ±å‡½æ•°çš„è¾“å‡ºå’Œæ€»å’Œåº”ç”¨æƒé‡
        rewards = (rewards_per_func * self.reward_weights.to(device).unsqueeze(0)).nansum(dim=1)

        # è®¡ç®—åˆ†ç»„æ™ºèƒ½å¥–åŠ±
        mean_grouped_rewards = rewards.view(-1, self.num_generations).mean(dim=1)
        std_grouped_rewards = rewards.view(-1, self.num_generations).std(dim=1)

        # å°†å¥–åŠ±æ ‡å‡†åŒ–ä»¥è®¡ç®—ä¼˜åŠ¿
        mean_grouped_rewards = mean_grouped_rewards.repeat_interleave(self.num_generations, dim=0)
        std_grouped_rewards = std_grouped_rewards.repeat_interleave(self.num_generations, dim=0)
        advantages = rewards - mean_grouped_rewards
        if self.args.scale_rewards:
            advantages = advantages / (std_grouped_rewards + 1e-4)

        # åˆ‡ç‰‡ä»…ä¿ç•™æ•°æ®çš„æœ¬åœ°éƒ¨åˆ†
        process_slice = slice(
            self.accelerator.process_index * len(prompts),
            (self.accelerator.process_index + 1) * len(prompts),
        )
        advantages = advantages[process_slice]

        # Log the metrics
        mode = "eval" if self.control.should_evaluate else "train"

        if mode == "train":
            self.state.num_input_tokens_seen += self.accelerator.gather_for_metrics(attention_mask.sum()).sum().item()
        self._metrics[mode]["num_tokens"] = [self.state.num_input_tokens_seen]

        # log completion lengths, mean, min, max
        agg_completion_mask = self.accelerator.gather_for_metrics(completion_mask.sum(1))
        self._metrics[mode]["completions/mean_length"].append(agg_completion_mask.float().mean().item())
        self._metrics[mode]["completions/min_length"].append(agg_completion_mask.float().min().item())
        self._metrics[mode]["completions/max_length"].append(agg_completion_mask.float().max().item())

        # è¯†åˆ«ä»¥EOSç»“å°¾çš„åºåˆ—å¹¶è®°å½•å…¶é•¿åº¦
        agg_terminated_with_eos = self.accelerator.gather_for_metrics(is_eos.any(dim=1))
        term_completion_mask = agg_completion_mask[agg_terminated_with_eos]
        clipped_completions_ratio = 1 - len(term_completion_mask) / len(agg_completion_mask)
        self._metrics[mode]["completions/clipped_ratio"].append(clipped_completions_ratio)
        if len(term_completion_mask) == 0:
            # æœªæ‰¾åˆ°å®Œæ•´åºåˆ—çš„è¾¹ç¼˜æƒ…å†µ
            term_completion_mask = torch.zeros(1, device=device)
        self._metrics[mode]["completions/mean_terminated_length"].append(term_completion_mask.float().mean().item())
        self._metrics[mode]["completions/min_terminated_length"].append(term_completion_mask.float().min().item())
        self._metrics[mode]["completions/max_terminated_length"].append(term_completion_mask.float().max().item())

        # è·å–å¥–åŠ±å‡½æ•°çš„åç§°
        reward_func_names = []
        for reward_func in self.reward_funcs:
            if isinstance(reward_func, nn.Module):  # æ¨¡å—è€Œä¸æ˜¯é¢„è®­ç»ƒæ¨¡å‹ï¼Œä»¥ä¸ç¼–è¯‘åçš„æ¨¡å‹å…¼å®¹
                reward_func_name = reward_func.config._name_or_path.split("/")[-1]
            else:
                reward_func_name = reward_func.__name__
            reward_func_names.append(reward_func_name)

        # è®¡ç®—æ¯ä¸ªå‡½æ•°çš„å¹³å‡å¥–åŠ±ï¼Œä½†ä»…é€‚ç”¨äºåº”ç”¨è¯¥å‡½æ•°çš„æ ·æœ¬ï¼ˆéNaNå€¼ï¼‰
        for i, reward_func_name in enumerate(reward_func_names):
            mean_rewards = torch.nanmean(rewards_per_func[:, i]).item()
            self._metrics[mode][f"rewards/{reward_func_name}/mean"].append(mean_rewards)
            std_rewards = nanstd(rewards_per_func[:, i]).item()
            self._metrics[mode][f"rewards/{reward_func_name}/std"].append(std_rewards)
        self._metrics[mode]["reward"].append(mean_grouped_rewards.mean().item())
        self._metrics[mode]["reward_std"].append(std_grouped_rewards.mean().item())

        # æ—¥å¿—æç¤ºå’Œå®Œæˆæ–‡æœ¬
        self._textual_logs["prompt"].extend(gather_object(prompts_text))
        self._textual_logs["completion"].extend(gather_object(completions_text))
        for i, name in enumerate(reward_func_names):
            self._textual_logs["rewards"][name].extend(rewards_per_func[:, i].tolist())

        return {
            "prompt_ids": prompt_ids,
            "prompt_mask": prompt_mask,
            "completion_ids": completion_ids,
            "completion_mask": completion_mask,
            "advantages": advantages,
            "old_per_token_logps": old_per_token_logps,
            "ref_per_token_logps": ref_per_token_logps,
        }

    def compute_liger_loss(self, model, inputs):
        # è®¡ç®—æ¨¡å‹çš„æ¯ä¸ªä»¤ç‰Œçš„æ—¥å¿—æ¦‚ç‡
        prompt_ids, prompt_mask = inputs["prompt_ids"], inputs["prompt_mask"]
        completion_ids, completion_mask = inputs["completion_ids"], inputs["completion_mask"]
        input_ids = torch.cat([prompt_ids, completion_ids], dim=1)
        attention_mask = torch.cat([prompt_mask, completion_mask], dim=1)
        logits_to_keep = completion_ids.size(1)  # æˆ‘ä»¬åªéœ€è¦è®¡ç®—å®Œæˆä»¤ç‰Œçš„logits

        # è·å–æ¨¡å‹çš„æœ€åä¸€ä¸ªéšè—çŠ¶æ€
        last_hidden_state = self._get_last_hidden_state(model, input_ids, attention_mask, logits_to_keep)
        unwrapped_model = self.accelerator.unwrap_model(model)
        # ä½¿ç”¨liger grpoæŸå¤±è®¡ç®—æŸå¤±å’Œåº¦é‡
        loss, metrics = self.liger_grpo_loss(
            _input=last_hidden_state,
            lin_weight=unwrapped_model.lm_head.weight,
            selected_token_ids=completion_ids,
            attention_mask=completion_mask,
            advantages=inputs["advantages"],
            bias=unwrapped_model.lm_head.bias,
            ref_per_token_logps=inputs["ref_per_token_logps"],
            old_per_token_logps=inputs["old_per_token_logps"],
        )
        # ä»liger_grpo_lossè¾“å‡ºä¸­æå–æŒ‡æ ‡
        # #å½“è´å¡”ä¸ºéé›¶æ—¶ï¼ŒKLæ•£åº¦æ˜¯ç¬¬ä¸€ä¸ªåº¦é‡
        mean_kl = metrics[0] if self.beta != 0.0 else None
        clip_ratio = metrics[-1]

        mode = "eval" if self.control.should_evaluate else "train"
        if self.beta != 0.0:
            self._metrics[mode]["kl"].append(self.accelerator.gather_for_metrics(mean_kl).mean().item())
        self._metrics[mode]["clip_ratio"].append(self.accelerator.gather_for_metrics(clip_ratio).mean().item())
        return loss

    @profiling_decorator
    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):
        if return_outputs:
            raise ValueError("The GRPOTrainer does not support returning outputs")
        if self.use_liger_loss:
            # ç”¨liger grpoæŸè€—è®¡ç®—æŸè€—
            return self.compute_liger_loss(model, inputs)
        else:
            return self._compute_loss(model, inputs)

    def _compute_loss(self, model, inputs):
        # Compute the per-token log probabilities for the model
        prompt_ids, prompt_mask = inputs["prompt_ids"], inputs["prompt_mask"]
        completion_ids, completion_mask = inputs["completion_ids"], inputs["completion_mask"]
        input_ids = torch.cat([prompt_ids, completion_ids], dim=1)
        attention_mask = torch.cat([prompt_mask, completion_mask], dim=1)
        logits_to_keep = completion_ids.size(1)  # æˆ‘ä»¬åªéœ€è¦è®¡ç®—å®Œæˆä»¤ç‰Œçš„logits

        per_token_logps = self._get_per_token_logps(model, input_ids, attention_mask, logits_to_keep)

        # è®¡ç®—æ¨¡å‹å’Œå‚è€ƒæ¨¡å‹ä¹‹é—´çš„KLæ•£åº¦
        if self.beta != 0.0:
            ref_per_token_logps = inputs["ref_per_token_logps"]
            per_token_kl = (
                torch.exp(ref_per_token_logps - per_token_logps) - (ref_per_token_logps - per_token_logps) - 1
            )

        # Compute the loss
        advantages = inputs["advantages"]
        # å½“ä½¿ç”¨num_iterations==1æ—¶ï¼Œold_per_token_logps==per_token_logsï¼Œå› æ­¤æˆ‘ä»¬å¯ä»¥è·³è¿‡å®ƒçš„è®¡ç®—ï¼ˆå‚è§
        # #_generate_and_score_collectionsï¼‰å¹¶ä½¿ç”¨per_token_logps.detachï¼ˆï¼‰ä»£æ›¿ã€‚
        old_per_token_logps = inputs["old_per_token_logps"] if self.num_iterations > 1 else per_token_logps.detach()
        coef_1 = torch.exp(per_token_logps - old_per_token_logps)
        coef_2 = torch.clamp(coef_1, 1 - self.epsilon_low, 1 + self.epsilon_high)
        per_token_loss1 = coef_1 * advantages.unsqueeze(1)
        per_token_loss2 = coef_2 * advantages.unsqueeze(1)
        per_token_loss = -torch.min(per_token_loss1, per_token_loss2)
        if self.beta != 0.0:
            per_token_loss = per_token_loss + self.beta * per_token_kl
        loss = (per_token_loss * completion_mask).sum() / completion_mask.sum()

        # Log the metrics
        mode = "eval" if self.control.should_evaluate else "train"

        if self.beta != 0.0:
            mean_kl = (per_token_kl * completion_mask).sum() / completion_mask.sum()
            self._metrics[mode]["kl"].append(self.accelerator.gather_for_metrics(mean_kl).mean().item())

        # Compute the clip ratio
        is_clipped = ((coef_1 < 1 - self.epsilon_low) & (advantages.unsqueeze(1) < 0)) | (
            (coef_1 > 1 + self.epsilon_high) & (advantages.unsqueeze(1) > 0)
        )
        clip_ratio = (is_clipped * completion_mask).sum() / completion_mask.sum()
        self._metrics[mode]["clip_ratio"].append(self.accelerator.gather_for_metrics(clip_ratio).mean().item())
        return loss

    def prediction_step(self, model, inputs, prediction_loss_only, ignore_keys: Optional[list[str]] = None):
        inputs = self._prepare_inputs(inputs)
        with torch.no_grad():
            with self.compute_loss_context_manager():
                loss = self.compute_loss(model, inputs)
            loss = loss.mean().detach()
        return loss, None, None

    def log(self, logs: dict[str, float], start_time: Optional[float] = None) -> None:
        mode = "eval" if self.control.should_evaluate else "train"
        metrics = {key: sum(val) / len(val) for key, val in self._metrics[mode].items()}  # average the metrics

        # è¿™ç§æ–¹æ³•æ—¢å¯ä»¥åœ¨è®­ç»ƒä¸­ä½¿ç”¨ï¼Œä¹Ÿå¯ä»¥åœ¨è¯„ä¼°ä¸­ä½¿ç”¨ã€‚
        # åœ¨è¯„ä¼°ä¸­è°ƒç”¨æ—¶ï¼Œ`logsä¸­çš„é”®`
        # #ä»¥â€œevalâ€å¼€å¤´ã€‚
        # æˆ‘ä»¬éœ€è¦åœ¨â€œmetricsâ€ä¸­çš„é”®å‰æ·»åŠ å‰ç¼€â€œeval_â€ä»¥åŒ¹é…æ ¼å¼ã€‚
        if mode == "eval":
            metrics = {f"eval_{key}": val for key, val in metrics.items()}

        logs = {**logs, **metrics}
        if version.parse(transformers.__version__) >= version.parse("4.47.0.dev0"):
            super().log(logs, start_time)
        else:  # transformers<=4.46
            super().log(logs)
        self._metrics[mode].clear()

        if self.accelerator.is_main_process:
            if is_rich_available():
                print_prompt_completions_sample(
                    self._textual_logs["prompt"],
                    self._textual_logs["completion"],
                    self._textual_logs["rewards"],
                    self.state.global_step,
                    self.num_completions_to_print,
                )

            if self.args.report_to and "wandb" in self.args.report_to and wandb.run is not None:
                import pandas as pd

                table = {
                    "step": [str(self.state.global_step)] * len(self._textual_logs["prompt"]),
                    "prompt": self._textual_logs["prompt"],
                    "completion": self._textual_logs["completion"],
                    **self._textual_logs["rewards"],
                }
                df = pd.DataFrame(table)
                if self.args.wandb_log_unique_prompts:
                    df = df.drop_duplicates(subset=["prompt"])
                wandb.log({"completions": wandb.Table(dataframe=df)})

    def create_model_card(
        self,
        model_name: Optional[str] = None,
        dataset_name: Optional[str] = None,
        tags: Union[str, list[str], None] = None,
    ):
        """
        Creates a draft of a model card using the information available to the `Trainer`.

        Args:
            model_name (`str` or `None`, *optional*, defaults to `None`):
                Name of the model.
            dataset_name (`str` or `None`, *optional*, defaults to `None`):
                Name of the dataset used for training.
            tags (`str`, `list[str]` or `None`, *optional*, defaults to `None`):
                Tags to be associated with the model card.
        """
        if not self.is_world_process_zero():
            return

        if hasattr(self.model.config, "_name_or_path") and not os.path.isdir(self.model.config._name_or_path):
            base_model = self.model.config._name_or_path
        else:
            base_model = None

        tags = tags or []
        if isinstance(tags, str):
            tags = [tags]

        if hasattr(self.model.config, "unsloth_version"):
            tags.append("unsloth")

        citation = textwrap.dedent(
            """\
            @article{zhihong2024deepseekmath,
                title        = {{DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models}},
                author       = {Zhihong Shao and Peiyi Wang and Qihao Zhu and Runxin Xu and Junxiao Song and Mingchuan Zhang and Y. K. Li and Y. Wu and Daya Guo},
                year         = 2024,
                eprint       = {arXiv:2402.03300},
            }
            """
        )

        model_card = generate_model_card(
            base_model=base_model,
            model_name=model_name,
            hub_model_id=self.hub_model_id,
            dataset_name=dataset_name,
            tags=tags,
            wandb_url=wandb.run.get_url() if is_wandb_available() and wandb.run is not None else None,
            comet_url=get_comet_experiment_url(),
            trainer_name="GRPO",
            trainer_citation=citation,
            paper_title="DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models",
            paper_id="2402.03300",
        )

        model_card.save(os.path.join(self.args.output_dir, "README.md"))